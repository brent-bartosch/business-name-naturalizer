import pLimit from 'p-limit';
import * as db from '../db/queries.js';
import { naturalizeNames } from './openrouter.js';
import { sendProcessingReport, sendErrorAlert } from './slack.js';

const BATCH_SIZE = parseInt(process.env.BATCH_SIZE) || 8;
const DELAY_BETWEEN_CALLS = parseInt(process.env.DELAY_BETWEEN_CALLS) || 1500;
const MAX_RECORDS_PER_RUN = parseInt(process.env.MAX_RECORDS_PER_RUN) || 1000;

/**
 * Process a batch of records to naturalize their names
 * @param {Object} options - Processing options
 * @returns {Promise<Object>} Processing statistics
 */
export async function processBatch(options = {}) {
  const startTime = Date.now();
  const stats = {
    processed: 0,
    naturalized: 0,
    from_cache: 0,
    api_calls: 0,
    errors: [],
    status: 'started'
  };

  try {
    console.log('üöÄ Starting batch processing...');
    
    // Get records to process
    const limit = options.limit || MAX_RECORDS_PER_RUN;
    const records = await db.getRecordsToProcess(limit);
    
    if (records.length === 0) {
      console.log('‚úÖ No records to process');
      stats.status = 'no_records';
      return stats;
    }

    console.log(`üìä Found ${records.length} records to process`);
    
    // Extract unique business names
    const uniqueNames = [...new Set(records.map(r => r.google_name))];
    console.log(`üîç ${uniqueNames.length} unique business names`);

    // Check cache for existing naturalizations
    const cachedNames = await db.getCachedNaturalNames(uniqueNames);
    const uncachedNames = uniqueNames.filter(name => !cachedNames[name]);
    
    stats.from_cache = Object.keys(cachedNames).length;
    console.log(`üíæ Found ${stats.from_cache} names in cache`);

    // Process uncached names in batches
    const naturalizedMap = { ...cachedNames };
    
    if (uncachedNames.length > 0) {
      console.log(`ü§ñ Processing ${uncachedNames.length} new names with AI...`);
      
      for (let i = 0; i < uncachedNames.length; i += BATCH_SIZE) {
        const batch = uncachedNames.slice(i, i + BATCH_SIZE);
        const batchNum = Math.floor(i / BATCH_SIZE) + 1;
        const totalBatches = Math.ceil(uncachedNames.length / BATCH_SIZE);
        
        console.log(`üì¶ Processing batch ${batchNum}/${totalBatches} (${batch.length} names)...`);
        
        try {
          // Call OpenRouter API
          const naturalNames = await naturalizeNames(batch);
          stats.api_calls++;
          
          // Map results
          const cacheEntries = [];
          for (let j = 0; j < batch.length; j++) {
            const originalName = batch[j];
            const naturalName = naturalNames[j] || originalName;
            
            naturalizedMap[originalName] = naturalName;
            cacheEntries.push({
              original_name: originalName,
              natural_name: naturalName
            });
          }
          
          // Save to cache
          await db.saveToCache(cacheEntries);
          stats.naturalized += cacheEntries.length;
          
          console.log(`‚úÖ Batch ${batchNum} complete`);
          
          // Rate limiting delay
          if (i + BATCH_SIZE < uncachedNames.length) {
            await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_CALLS));
          }
          
        } catch (error) {
          console.error(`‚ùå Failed to process batch ${batchNum}:`, error.message);
          stats.errors.push(`Batch ${batchNum}: ${error.message}`);
          
          // Use original names as fallback
          for (const name of batch) {
            naturalizedMap[name] = name;
          }
        }
      }
    }

    // Update database records with natural names
    console.log('üíæ Updating database records...');
    
    const updates = records.map(record => ({
      place_id: record.place_id,
      natural_name: naturalizedMap[record.google_name] || record.google_name
    }));
    
    await db.updateRecordsWithNaturalNames(updates);
    stats.processed = updates.length;
    
    // Get final statistics
    const finalStats = await db.getProcessingStats();
    stats.pending_records = finalStats.pending_records;
    
    // Calculate duration
    const duration = Math.round((Date.now() - startTime) / 1000);
    stats.duration = `${duration}s`;
    stats.status = 'completed';
    
    // Log to database
    await db.logProcessingResult({
      ...stats,
      timestamp: new Date().toISOString()
    });
    
    console.log('\n‚úÖ Processing complete!');
    console.log(`üìä Processed: ${stats.processed} records`);
    console.log(`ü§ñ Naturalized: ${stats.naturalized} new names`);
    console.log(`üíæ From cache: ${stats.from_cache} names`);
    console.log(`üìû API calls: ${stats.api_calls}`);
    console.log(`‚è±Ô∏è  Duration: ${stats.duration}`);
    
    if (stats.pending_records > 0) {
      console.log(`üìã Still pending: ${stats.pending_records} records`);
    }
    
    // Send Slack report
    await sendProcessingReport(stats);
    
    return stats;
    
  } catch (error) {
    console.error('‚ùå Fatal error in batch processing:', error);
    stats.status = 'error';
    stats.errors.push(error.message);
    
    // Send error alert
    await sendErrorAlert(error, {
      trigger: options.trigger || 'manual',
      records_affected: stats.processed
    });
    
    throw error;
  }
}

/**
 * Process records triggered by database event
 */
export async function processTriggeredRecords() {
  console.log('üîî Processing triggered by database event');
  
  const needsProcessing = await db.checkIfNaturalizationNeeded();
  
  if (needsProcessing) {
    console.log('‚úÖ New records detected, starting processing...');
    return await processBatch({ trigger: 'database' });
  } else {
    console.log('‚ÑπÔ∏è  No pending triggers found');
    return { status: 'no_triggers' };
  }
}

/**
 * Test processing with a small batch
 */
export async function testProcessing(limit = 10) {
  console.log(`üß™ Testing with ${limit} records...`);
  return await processBatch({ 
    limit, 
    trigger: 'test' 
  });
}